{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "ds = datasets.load_digits()\n",
    "X, y = ds['images'], ds['target']\n",
    "X = X / 255"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "(1797, 8, 8)"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "Array([1.1384060e-03, 8.0636370e-01, 5.8803633e-02, 2.2045203e-02,\n       9.5193041e-03, 4.3360114e-02, 1.1978690e-03, 4.7167134e-02,\n       9.9833654e-03, 4.2123359e-04], dtype=float32)"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = (\n",
    "\tnp.random.normal(scale=1 / 256, size=(64 + 10 + 1, 256)).T,  #   256\n",
    "\tnp.random.normal(scale=1 / 256, size=(256 + 1, 256)).T,  #   256\n",
    "\tnp.random.normal(scale=1 / 256, size=(256 + 1, 256)).T,  #   256\n",
    "\tnp.random.normal(scale=1 / 256, size=(256 + 1, 256)).T,  #   256\n",
    "\tnp.random.normal(scale=1 / 256, size=(256 + 1, 128)).T,  # + 128\n",
    "\t# ------\n",
    "\tnp.random.normal(size=(10, 1152 + 1))  #  1152\n",
    ")\n",
    "\n",
    "\n",
    "def layer_infer(layer, x):\n",
    "\tx = layer @ jnp.concatenate((x, np.ones(shape=1)))\n",
    "\tx = jnp.log1p(jnp.exp(x))  # softplus\n",
    "\tx = x / np.linalg.norm(x)  # layer norm\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def last_layer_infer(layer, x):\n",
    "\tx = layer @ jnp.concatenate((x, np.ones(shape=1)))\n",
    "\n",
    "\texp_x = jnp.exp(x)\n",
    "\treturn exp_x / np.sum(exp_x)  # softmax\n",
    "\n",
    "\n",
    "def inference(x, layers):\n",
    "\tlayer0, layer1, layer2, layer3, layer4, layerFinal = layers\n",
    "\n",
    "\tx0 = layer_infer(layer0, x)\n",
    "\tx1 = layer_infer(layer1, x0)\n",
    "\tx2 = layer_infer(layer2, x1)\n",
    "\tx3 = layer_infer(layer3, x2)\n",
    "\tx4 = layer_infer(layer4, x3)\n",
    "\n",
    "\treturn last_layer_infer(layerFinal, np.concatenate((x0, x1, x2, x3, x4)))\n",
    "\n",
    "\n",
    "inference(np.concatenate((X[0].flatten(), np.zeros(10))), parameters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "((Array([[  -0.       ,   -0.       ,   -3.4790595, ...,   -0.       ,\n            -0.       , -177.43202  ],\n         [  -0.       ,   -0.       ,   -3.4836123, ...,   -0.       ,\n            -0.       , -177.66422  ],\n         [  -0.       ,   -0.       ,   -3.4757967, ...,   -0.       ,\n            -0.       , -177.26562  ],\n         ...,\n         [  -0.       ,   -0.       ,   -3.472893 , ...,   -0.       ,\n            -0.       , -177.11754  ],\n         [  -0.       ,   -0.       ,   -3.4796553, ...,   -0.       ,\n            -0.       , -177.46242  ],\n         [  -0.       ,   -0.       ,   -3.471619 , ...,   -0.       ,\n            -0.       , -177.05255  ]], dtype=float32),\n  Array([[ -11.112507 ,  -11.133505 ,  -11.097481 , ...,  -11.115258 ,\n           -11.078258 , -177.75125  ],\n         [ -11.127423 ,  -11.148449 ,  -11.112376 , ...,  -11.1301775,\n           -11.093127 , -177.98984  ],\n         [ -11.120061 ,  -11.141073 ,  -11.105024 , ...,  -11.122814 ,\n           -11.085789 , -177.87209  ],\n         ...,\n         [ -11.092246 ,  -11.113206 ,  -11.077248 , ...,  -11.094992 ,\n           -11.05806  , -177.42717  ],\n         [ -11.135517 ,  -11.156558 ,  -11.12046  , ...,  -11.138273 ,\n           -11.101196 , -178.11931  ],\n         [ -11.102686 ,  -11.123666 ,  -11.087673 , ...,  -11.105434 ,\n           -11.068467 , -177.59416  ]], dtype=float32),\n  Array([[ -11.100711 ,  -11.122232 ,  -11.111607 , ...,  -11.133926 ,\n           -11.086549 , -177.36487  ],\n         [ -11.083474 ,  -11.104962 ,  -11.094353 , ...,  -11.116638 ,\n           -11.069334 , -177.08946  ],\n         [ -11.054589 ,  -11.076021 ,  -11.06544  , ...,  -11.0876665,\n           -11.040486 , -176.62794  ],\n         ...,\n         [ -11.076315 ,  -11.097789 ,  -11.087187 , ...,  -11.109457 ,\n           -11.062183 , -176.97507  ],\n         [ -11.057281 ,  -11.078718 ,  -11.068133 , ...,  -11.090366 ,\n           -11.043174 , -176.67094  ],\n         [ -11.068109 ,  -11.089567 ,  -11.078972 , ...,  -11.101226 ,\n           -11.0539875, -176.84395  ]], dtype=float32),\n  Array([[ -11.098392 ,  -11.073548 ,  -11.032004 , ...,  -11.035872 ,\n           -11.051435 , -177.51892  ],\n         [ -11.132752 ,  -11.107833 ,  -11.06616  , ...,  -11.07004  ,\n           -11.085651 , -178.06853  ],\n         [ -11.109155 ,  -11.084288 ,  -11.042704 , ...,  -11.046576 ,\n           -11.062154 , -177.69109  ],\n         ...,\n         [ -11.102821 ,  -11.077969 ,  -11.036408 , ...,  -11.0402775,\n           -11.055847 , -177.58978  ],\n         [ -11.108062 ,  -11.083198 ,  -11.041616 , ...,  -11.045488 ,\n           -11.061065 , -177.6736   ],\n         [ -11.1505165,  -11.125557 ,  -11.0838175, ...,  -11.087704 ,\n           -11.10334  , -178.35266  ]], dtype=float32),\n  Array([[ -5.516039 ,  -5.540717 ,  -5.523762 , ...,  -5.5229764,\n           -5.5535045, -88.28999  ],\n         [ -5.5448103,  -5.5696173,  -5.5525737, ...,  -5.551784 ,\n           -5.5824714, -88.75051  ],\n         [ -5.5540676,  -5.578916 ,  -5.561844 , ...,  -5.5610533,\n           -5.591791 , -88.89868  ],\n         ...,\n         [ -5.5582423,  -5.5831094,  -5.5660243, ...,  -5.5652328,\n           -5.595994 , -88.9655   ],\n         [ -5.522487 ,  -5.547194 ,  -5.530219 , ...,  -5.529433 ,\n           -5.559996 , -88.3932   ],\n         [ -5.5331345,  -5.557889 ,  -5.540881 , ...,  -5.5400934,\n           -5.570716 , -88.56362  ]], dtype=float32),\n  Array([[-0., -0., -0., ..., -0., -0., -0.],\n         [-0., -0., -0., ..., -0., -0., -0.],\n         [-0., -0., -0., ..., -0., -0., -0.],\n         ...,\n         [-0., -0., -0., ..., -0., -0., -0.],\n         [-0., -0., -0., ..., -0., -0., -0.],\n         [-0., -0., -0., ..., -0., -0., -0.]], dtype=float32)),\n Array([1.1384060e-03, 8.0636370e-01, 5.8803633e-02, 2.2045203e-02,\n        9.5193041e-03, 4.3360114e-02, 1.1978690e-03, 4.7167134e-02,\n        9.9833654e-03, 4.2123359e-04], dtype=float32))"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_infer_0(layer, x):\n",
    "\tx = layer @ jnp.concatenate((x, np.ones(shape=1)))\n",
    "\tx = jnp.log1p(jnp.exp(x))  # softplus\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def layer_train(layer, x, positive):\n",
    "\tdlayer = jax.grad(lambda l, x: jnp.sum(layer_infer_0(l, x)) ** 2)(layer, x)\n",
    "\tif positive:\n",
    "\t\tdlayer *= -1\n",
    "\treturn dlayer, layer_infer(layer, x)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred, y):\n",
    "\treturn jnp.sum(-y * jnp.log(y_pred))\n",
    "\n",
    "\n",
    "def last_layer_train(layer, x, y, positive):\n",
    "\treturn jax.grad(lambda l, x, y: cross_entropy_loss(last_layer_infer(l, x), y))(layer, x,\n",
    "\t                                                                               y) if positive else jnp.zeros_like(\n",
    "\t\tlayer), last_layer_infer(layer, x)\n",
    "\n",
    "\n",
    "def train(x, y, layers, positive: bool):\n",
    "\tlayer0, layer1, layer2, layer3, layer4, layerFinal = layers\n",
    "\tdl0, x0 = layer_train(layer0, x, positive)\n",
    "\tdl1, x1 = layer_train(layer1, x0, positive)\n",
    "\tdl2, x2 = layer_train(layer2, x1, positive)\n",
    "\tdl3, x3 = layer_train(layer3, x2, positive)\n",
    "\tdl4, x4 = layer_train(layer4, x3, positive)\n",
    "\n",
    "\tdlf, y = last_layer_train(layerFinal, jnp.concatenate((x0, x1, x2, x3, x4)), y, positive)\n",
    "\treturn (dl0, dl1, dl2, dl3, dl4, dlf), y\n",
    "\n",
    "\n",
    "train(np.concatenate((X[0].flatten(), np.zeros(10))), np.zeros(10), parameters, positive=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 start!\n",
      "Total Loss 7.771260738372803\n",
      "Epoch 0 end!\n",
      "Epoch 1 start!\n",
      "Total Loss 7.289566993713379\n",
      "Epoch 1 end!\n",
      "Epoch 2 start!\n",
      "Total Loss 6.918318271636963\n",
      "Epoch 2 end!\n",
      "Epoch 3 start!\n",
      "Total Loss 6.630514621734619\n",
      "Epoch 3 end!\n",
      "Epoch 4 start!\n",
      "Total Loss 6.39785099029541\n"
     ]
    }
   ],
   "source": [
    "\n",
    "theta = parameters\n",
    "for i in range(10):\n",
    "\tprint(f\"Epoch {i} start!\")\n",
    "\ttotal_loss = 0\n",
    "\tfor x, y in zip(X_test, y_test):\n",
    "\t\ttotal_loss = cross_entropy_loss(inference(np.concatenate((x.flatten(), np.ones(10) / 10)), theta), np.eye(10)[y])\n",
    "\tprint(f\"Total Loss {total_loss}\")\n",
    "\n",
    "\tfor x, y in zip(X_train, y_train):\n",
    "\t\tpositive = np.concatenate((x.flatten(), np.eye(10)[y]))\n",
    "\t\tneg_y = y\n",
    "\t\twhile neg_y != y:\n",
    "\t\t\tneg_y = random.randint(0, 9)\n",
    "\t\tnegative = np.concatenate((x.flatten(), np.eye(10)[neg_y]))\n",
    "\n",
    "\t\tdtheta_p, _ = train(positive, np.eye(10)[y], theta, positive=True)\n",
    "\t\tdtheta_n, _ = train(negative, np.eye(10)[neg_y], theta, positive=False)\n",
    "\t\ttheta = tuple(theta - dtp * .0001 - dtn * .0001 for theta, dtp, dtn in zip(theta, dtheta_p, dtheta_n))\n",
    "\n",
    "\tprint(f\"Epoch {i} end!\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X[0], y[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# It works! yay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
